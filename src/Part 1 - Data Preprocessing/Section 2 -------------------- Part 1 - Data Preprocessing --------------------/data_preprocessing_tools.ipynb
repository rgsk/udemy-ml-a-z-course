{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### independent vs dependent variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generally, first columns of the dataset are independent variables or features, these are the columns with which we are going to predict the dependent variable.\n",
    "\n",
    "dependent variables are last columns, they are outputs based on features.\n",
    "\n",
    "in given dataset Data.csv\n",
    "\n",
    "Country, Age, Salary are features and last column Purchased is the outcome, \n",
    "\n",
    "so given the country, age and salary of a customer whether purchase was made or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using pandas to import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Data.csv')\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking care of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer.fit(X[:, 1:3])\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mean is returned by sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prac_X[:,1]: [44.0 27.0 30.0 38.0 40.0 35.0 nan 48.0 50.0 37.0]\n",
      "ages: [44.0, 27.0, 30.0, 38.0, 40.0, 35.0, 48.0, 50.0, 37.0]\n",
      "mean(ages): 38.77777777777778\n"
     ]
    }
   ],
   "source": [
    "prac_X = dataset.iloc[:, :-1].values\n",
    "print(\"prac_X[:,1]:\", prac_X[:, 1])\n",
    "ages = [v for v in prac_X[:, 1] if str(v) != 'nan']\n",
    "print(\"ages:\", ages)\n",
    "print(\"mean(ages):\", np.mean(ages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prac_X[:,2]: [72000.0 48000.0 54000.0 61000.0 nan 58000.0 52000.0 79000.0 83000.0\n",
      " 67000.0]\n",
      "salaries: [72000.0, 48000.0, 54000.0, 61000.0, 58000.0, 52000.0, 79000.0, 83000.0, 67000.0]\n",
      "mean(salaries): 63777.77777777778\n"
     ]
    }
   ],
   "source": [
    "print(\"prac_X[:,2]:\", prac_X[:, 2])\n",
    "salaries = [v for v in prac_X[:, 2] if str(v) != 'nan']\n",
    "print(\"salaries:\", salaries)\n",
    "print(\"mean(salaries):\", np.mean(salaries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How multiple nan are handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "all nan are replaced by mean, presence of more nan has no impact, mean is calculated as follows\n",
    "\n",
    "sum of valid values / number of valid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marks: [[ 1.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [nan]\n",
      " [ 4.]\n",
      " [nan]\n",
      " [ 5.]\n",
      " [nan]]\n",
      "transformation...\n",
      "marks: [[1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [5.]\n",
      " [3.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "marks = np.array([1, 2, 3, np.nan, 4, np.nan, 5, np.nan])\n",
    "marks = marks.reshape(len(marks), 1)\n",
    "print(\"marks:\", marks)\n",
    "imputer.fit(marks)\n",
    "marks = imputer.transform(marks)\n",
    "print('transformation...')\n",
    "print(\"marks:\", marks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using other methods for filling the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marks: [[ 1.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [nan]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 6.]]\n",
      "transformation...\n",
      "marks: [[1. ]\n",
      " [2. ]\n",
      " [3. ]\n",
      " [3.5]\n",
      " [4. ]\n",
      " [5. ]\n",
      " [6. ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "marks = np.array([1, 2, 3, np.nan, 4, 5, 6])\n",
    "marks = marks.reshape(len(marks), 1)\n",
    "print(\"marks:\", marks)\n",
    "imputer.fit(marks)\n",
    "marks = imputer.transform(marks)\n",
    "print('transformation...')\n",
    "print(\"marks:\", marks)\n",
    "\n",
    "# median is taken (3+4)/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marks: [[ 1.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [nan]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 6.]\n",
      " [ 7.]\n",
      " [ 5.]\n",
      " [nan]]\n",
      "transformation...\n",
      "marks: [[1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [5.]\n",
      " [4.]\n",
      " [5.]\n",
      " [6.]\n",
      " [7.]\n",
      " [5.]\n",
      " [5.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "# 5 is specified twice\n",
    "marks = np.array([1, 2, 3, np.nan, 4, 5, 6, 7, 5, np.nan])\n",
    "marks = marks.reshape(len(marks), 1)\n",
    "print(\"marks:\", marks)\n",
    "imputer.fit(marks)\n",
    "marks = imputer.transform(marks)\n",
    "print('transformation...')\n",
    "print(\"marks:\", marks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marks: [[ 1.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [nan]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 6.]\n",
      " [nan]\n",
      " [ 7.]\n",
      " [ 5.]]\n",
      "transformation...\n",
      "marks: [[ 1.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [10.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 6.]\n",
      " [10.]\n",
      " [ 7.]\n",
      " [ 5.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=10) # fill_value=0 by default\n",
    "marks = np.array([1, 2, 3, np.nan, 4, 5, 6, np.nan, 7, 5])\n",
    "marks = marks.reshape(len(marks), 1)\n",
    "print(\"marks:\", marks)\n",
    "imputer.fit(marks)\n",
    "marks = imputer.transform(marks)\n",
    "print('transformation...')\n",
    "print(\"marks:\", marks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the Independent Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why simply assigning numerically ordered values to countries is bad?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "currently the Country column contains France, Spain, Germany\n",
    "\n",
    "now we want to turn strings into numerical values,\n",
    "\n",
    "one idea would be to encode France into 0, Spain into 1 and Germany into 2\n",
    "\n",
    "but by following above approach, our model could interpret that \n",
    "\n",
    "there is a numerical order between these three countries, and it could interpret that this order matters whereas of course it is not the case.\n",
    "\n",
    "There is not a relationship order between these 3 countries.\n",
    "\n",
    "So, we want to avoid the model to have such an interpretation because that could cause some misinterpreted correlations between features and the outcome which we want to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one hot encoding consists of creating binary vectors for each of the countries.\n",
    "\n",
    "since data contains 3 different countries, it will create 3 different columns and assignment will be like \n",
    "\n",
    "France -> 100\n",
    "\n",
    "Spain -> 010\n",
    "\n",
    "Germany -> 001\n",
    "\n",
    "this way each country got uniquely identified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough') \n",
    "X = np.array(ct.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 35.0 58000.0]\n",
      " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "('encoder', OneHotEncoder(), [0])\n",
    "\n",
    "'encoder' -> kind of transformation\n",
    "\n",
    "OneHotEncoder() -> class that will proceed to this encoding\n",
    "\n",
    "[0] -> columns we want to apply one hot encoding to\n",
    "\n",
    "___\n",
    "\n",
    "\n",
    "remainder='passthrough' -> passthrough rest of the columns (don't modify them)\n",
    "\n",
    "___\n",
    "\n",
    "`ct.fit_transform(X)` doesn't returns output as a NumPy array, we need X to be a numpy array for future machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "performing one hot encoding on other column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['22' 'rahul']\n",
      " ['19' 'mehak']]\n",
      "[['0.0' '1.0' '22']\n",
      " ['1.0' '0.0' '19']]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "#                                                           column 1 contains name\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder='passthrough')\n",
    "names = np.array([[22, 'rahul'], [19, 'mehak']])\n",
    "print(names)\n",
    "names = np.array(ct.fit_transform(names))\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we don't pass remainder='passthrough', we would only remian with columns resulting from one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['22' 'rahul']\n",
      " ['19' 'mehak']]\n",
      "[[0. 1.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])])\n",
    "names = np.array([[22, 'rahul'], [19, 'mehak']])\n",
    "print(names)\n",
    "names = np.array(ct.fit_transform(names))\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "names = ['rahul', 'mehak', 'rahul', 'mehak']\n",
    "names = le.fit_transform(names)\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 2 0 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "# LabelEncoder assigns numerically ordered values\n",
    "# but it was useful for our Dataset as it contained only Yes, No\n",
    "names = ['rahul', 'mehak', 'rahul', 'aman', 'raja']\n",
    "names = le.fit_transform(names)\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use one hot encoding when you have several categories in one of the features of your matrix of features\n",
    "\n",
    "but also you can do a simple lable encoding when you have, two classes which you can directly encode into zero and onces, ie. a binary outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 35.0 58000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 30.0 54000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`test_size = 0.2`\n",
    "\n",
    "signifies the percentage of entries going into test set\n",
    " \n",
    "generally 80:20 is a good split\n",
    "___\n",
    "\n",
    "passing\n",
    "\n",
    "`random_state = 1`\n",
    "\n",
    "ensures that `train_test_split` function returns the same entries in training and test set, every time it is ran\n",
    "\n",
    "it also ensured that split used by instructor is same as ours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names_train: [['raja'], ['mehak'], ['rahul']]\n",
      "names_test: [['aman']]\n",
      "ages_train: [20, 19, 23]\n",
      "ages_test: [23]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "names = [\n",
    "    ['rahul'],\n",
    "    ['mehak'],\n",
    "    ['aman'],\n",
    "    ['raja']\n",
    "]\n",
    "ages = [\n",
    "    23, \n",
    "    19, \n",
    "    23,\n",
    "    20\n",
    "]\n",
    "# value of random_state doesn't matters as long as it is same everytime\n",
    "names_train, names_test, ages_train, ages_test = train_test_split(names, ages, test_size = 0.2, random_state = 2)\n",
    "print(\"names_train:\",names_train)\n",
    "print(\"names_test:\",names_test)\n",
    "print(\"ages_train:\",ages_train)\n",
    "print(\"ages_test:\",ages_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names_train: [['aman'], ['raja']]\n",
      "names_test: [['rahul'], ['mehak']]\n",
      "ages_train: [23, 20]\n",
      "ages_test: [23, 19]\n"
     ]
    }
   ],
   "source": [
    "names_train, names_test, ages_train, ages_test = train_test_split(names, ages, test_size = 0.5)\n",
    "print(\"names_train:\",names_train)\n",
    "print(\"names_test:\",names_test)\n",
    "print(\"ages_train:\",ages_train)\n",
    "print(\"ages_test:\",ages_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What it is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature scaling simply consists of scaling all features to make sure they all take values in the same scale.\n",
    "\n",
    "And we do this so as to prevent one feature to dominate the other, which therefore would be neglected by the machine learning model.\n",
    "\n",
    "___\n",
    "\n",
    "feature scaling is required for some ml models,\n",
    "\n",
    "for eg. for Regression based models -> \n",
    "\n",
    "Multiple Linear Regression \n",
    "\n",
    "y = b0 + b1 * x1 + b2 * x2 + ... + bn * xn\n",
    "\n",
    "cofficients would compensate for features having values at different scales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature scaling methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](assets/feature-scaling-methods.jpeg)\n",
    "\n",
    "Standardisation method will put all the values in the feature between -2 and 2\n",
    "for, python implementation its -2 and 2\n",
    "\n",
    "Normalisation method will make all the values ranging between 0 and 1\n",
    "\n",
    "because x - min(x) > 0 and max(x) - min(x) > 0, so positive / positive also denomenator > numerator, so value will be less than equal to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which method to use??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Normalisation is good when most of the features are following normal distribution\n",
    "\n",
    "Standardisation will do the job will work all the time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will use Standardisation below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: don't apply feature scaling to dummy variables (oneHotEncoded variables), as they are already between the range of 0-1, so they don't need to be scaled, moreover, if we apply feature scaling to them, then would take values in range -2,2. but that completely defeat the purpose of oneHotEncoding, which assigned vectors to uniquely identify each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "X_train_before_scaling = X_train.copy()\n",
    "X_test_before_scaling = X_test.copy()\n",
    "\n",
    "# cols after Country OneHotEncoded columns\n",
    "# fit will get the mean and starndard deviation, transform will apply the method to each of the values\n",
    "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\n",
    "\n",
    "\n",
    "# sc.fit_transform is not called below\n",
    "# reason ->\n",
    "# features of the test set need to be scaled by the same scaler that was used on the training set.\n",
    "# machine learning model will be trained with scaler applied on the training set, in order to make \n",
    "# predictions that will be congruent with the way the model was trained, we need to apply the same scaler\n",
    "# that was used on the training set onto the test set, so that we can get indeed the same transformation. \n",
    "# and therefore, in the end, some relevant predictions with the predict method applied to x_test.\n",
    "X_test[:, 3:] = sc.transform(X_test[:, 3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
      " [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
      " [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n",
      " [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
      " [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
      " [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n",
      " [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
      " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
      " [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.41421356],\n",
       "       [-0.70710678],\n",
       "       [ 0.        ],\n",
       "       [ 0.70710678],\n",
       "       [ 1.41421356]])"
      ]
     },
     "execution_count": 681,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "values = np.array([1, 2, 3, 4, 5])\n",
    "values = values.reshape(len(values), 1)\n",
    "sc.fit_transform(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 3.0\n",
      "std: 1.4142135623730951\n",
      "scaled_values: [-1.414213562373095, -0.7071067811865475, 0.0, 0.7071067811865475, 1.414213562373095]\n"
     ]
    }
   ],
   "source": [
    "values = np.array([1, 2, 3, 4, 5])\n",
    "mean = values.mean()\n",
    "std = values.std()\n",
    "print(\"mean:\",mean)\n",
    "print(\"std:\",std)\n",
    "scaled_values = [(v - mean)/std for v in values]\n",
    "print(\"scaled_values:\",scaled_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.414213562373095, -0.7071067811865475, 0.0, 0.7071067811865475, 1.414213562373095], 3.0, 1.4142135623730951]\n",
      "X_train_before_scaling: [[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 35.0 58000.0]]\n",
      "[-0.19159184384578545, -0.014117293757057777, 0.566708506533324, -0.30453019390224867, -1.9018011447007988, 1.1475343068237058, 1.4379472069688968, -0.7401495441200351]\n",
      "[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
      " [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
      " [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n",
      " [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
      " [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
      " [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n",
      " [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
      " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n",
      "[-1.4661817944830124, -0.44973664397484414]\n",
      "[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
      " [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
     ]
    }
   ],
   "source": [
    "def custom_scaler(values: np.ndarray):\n",
    "    mean = values.mean()\n",
    "    std = values.std()\n",
    "    scaled_values = [(v - mean)/std for v in values]\n",
    "    return [scaled_values, mean, std]\n",
    "\n",
    "\n",
    "print(custom_scaler(np.array([1, 2, 3, 4, 5])))\n",
    "print(\"X_train_before_scaling:\", X_train_before_scaling)\n",
    "scaled_values, mean, std = custom_scaler(X_train_before_scaling[:, 3])\n",
    "print(scaled_values)\n",
    "print(X_train)\n",
    "\n",
    "print([(v - mean)/std for v in X_test_before_scaling[:, 3]])\n",
    "print(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Splitting before Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the test set is supposed to be a brand new set on which you're going to evaluate machine learning model. \n",
    "\n",
    "If feature scaling is applied before splitting, then your machine learning model will be training on your test set.\n",
    "\n",
    "But test set is not supposted to be worked with for the traning, it should remain completely untouched by the model. (should remain completely new to the model so it can be used to determine accuracy of the model)\n",
    "\n",
    "feature scaling will get the mean and standard deviation of your feature, in order to perform the scaling.\n",
    "\n",
    "So, if we apply feature scaling before the split, then it will get the mean and the standard deviation of all the values, including the one's in the test set.\n",
    "\n",
    "So, applying Feature Scaling before Splitting will cause some information leakage on the test set. \n",
    "\n",
    "we want our model to be tested on completely new observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you should not apply feature scaling before the split to prevent information leakage on the test set, which you are not supposed to have until the training is done."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('udemy-venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35e8025d8ebc2db52173a220a522dbc89b3de1433bce65aa1c566ead1ed78455"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
